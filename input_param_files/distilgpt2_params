'''
Vineet Kumar, sioom.ai

This input file has a unique set of user-programmable hyper-parameters for
training and testing a model. It is envisioned that there will be many input
files, in the "input_param_files" directory, each with their own unique set
of hyperparameters.

Note the following:
	(1) This file should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "conversational-transaction-bot"
 
Command-line:
-------------
python3 ctbMain.py input_param_files/distilgpt2_params 

Path to ctb logs files:
-----------------------
It is the default directory, and the name of the file is "ctb_logs".

Path to Lightning logs files:
-----------------------------
It includes the following directories: (i) ctb_lightning_logs, (ii) name
of this file, (iii) a unique version number that increases every time this
file is used for training. Following is an example:
ctb_lightning_logs/distilgpt2_params/version_0
where "distilgpt2_params" is the name of this file

Path to Checkpointed files:
---------------------------
It includes the following directories: (i) path of Lightning logs files,
(ii) checkpoints. Following is an example:
ctb_lightning_logs/distilgpt2_params/version_0/checkpoints
where "distilgpt2_params" is the name of this file

Name of Checkpointed files:
---------------------------
During training, the checkpointed files are those that have the lowest
validation loss. The name includes the epoch number plus the value of the
validation loss. Following is an example:
epoch=00-val_loss=0.2329.ckpt
'''


# parameters for file "ctbMain.py"
# save_top_k: number of checkpointed files to save
{'save_top_k': 2}

# parameters for file "ctbModel.py"
# model_type: name of pretrained model plus name of training dataset
# tokenizer_type: name of pretrained tokenizer plus name of training
#                 dataset whose tokens are added
{'model_type': 'distilgpt2-dstc2', 'tokenizer_type': 'gpt2-dstc2'} 

# parameters for file "ctbData.py"
{'default_format_path': 'data/dialog-bAbI-tasks/dstc2/defaultFormat.train', 'batch_size_train': 2, 'batch_size_val': 2, 'batch_size_test': 2} 

# parameters for Lightning Trainer
{'gpus': 1, 'max_epochs': 1}

