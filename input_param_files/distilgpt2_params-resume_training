Vineet Kumar, sioom.ai

This input file has user-settable hyper-parameters to resume training of
   a checkpointed model. 

Note the following:
	(1) This file should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "conversational-transaction-bot"
 
Command-line:
-------------
python3 ctbMain.py input_param_files/distilgpt2_params-resume_training 


parameters for file "ctbMain.py"
- 'save_top_k':    number of checkpointed files to save		Default: 1
- 'no_testing':    whether to not-test a trained model		Default: False
- 
{'save_top_k': 2, 'no_testing': True}


parameters for file "ctbModel.py"
- Parameters MUST be the same as those in the model from where training is resumed
- 'model_type':      'name of pretrained model plus name of training dataset'
- 'tokenizer_type':  'name of pretrained tokenizer plus name of training
                         dataset whose tokens are added'
- 'optz':            'name of optimizer'
- 'optz_params':     {parameters of optimizer EXCLUDING "params"}
- 'lr_sched':        'name of lr-scheduler'
- 'lr_sched_params': {parameters of scheduler EXCLUDING "optimizer"}
- eu = euler's number; e = exponent with a base of 10
- eu1 =   2.71828182846     eu-1 =  3.6787944117e-1     eu-2 =  1.3533528323e-1
- eu-3 =  4.978706836e-2    eu-4 =  1.831563888e-2      eu-5 =  6.73794699e-3
- eu-6 =  2.47875217e-3     eu-7 =  9.1188196e-4        eu-8 =  3.3546262e-4
- eu-9 =  1.234098e-4       eu-10 = 4.539992e-5         eu-11 = 1.67017e-5 
- eu-12 = 6.14421e-6
- 
{'model_type': 'distilgpt2-dstc2', 'tokenizer_type': 'gpt2-dstc2', 'optz': 'Adam', 'optz_params': {'lr': 0.9}, 'lr_sched': 'ReduceLROnPlateau', 'lr_sched_params': {'mode': 'min', 'patience': 1, 'verbose': True}} 
#{'model_type': 'distilgpt2-dstc2', 'tokenizer_type': 'gpt2-dstc2', 'optz': 'SGD', 'optz_params': {'lr': 0.8}, 'lr_sched': 'CyclicLR', 'lr_sched_params': {'base_lr': 0.0001, 'max_lr': 0.1}} 


parameters for file "ctbData.py"
- 'default_format_path': 'path to training dataset'
- 
{'default_format_path': 'data/dialog-bAbI-tasks/dstc2/defaultFormat.train', 'batch_size': 2} 


parameters for Lightning Trainer
- For a list of parameters, see Trainer.__init__(...) in PyTorch Lightning documentation
- The Model and Training state is restored during "trainer.fit()", therefore
     "trainer.tune()" is useless. This means, do not use 'auto_lr_find' and 
     'auto_scale_batch_size'
- 
{'gpus': 1, 'resume_from_checkpoint': 'tensorboard_logs/distilgpt2_params-resume_training/version_10/checkpoints/last.ckpt'}

