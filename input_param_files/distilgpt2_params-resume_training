Vineet Kumar, sioom.ai

This input file has a unique set of user-settable hyper-parameters to resume 
training of a checkpointed model. It is envisioned that there will be many input
files, in the "input_param_files" directory, each with their own unique set
of hyperparameters.

Note the following:
	(1) This file should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "conversational-transaction-bot"
 
Command-line:
-------------
python3 ctbMain.py input_param_files/distilgpt2_params-resume_training 

Path to ctb logs files:
-----------------------
It is the default directory, and the name of the file is "ctb_logs".

Path to TensorBoard logs files:
-----------------------------
It includes the following directories: (i) tensorboard_logs, (ii) name
of this file, (iii) a unique version number that increases every time this
file is used for training. Following is an example:
tensorboard_logs/distilgpt2_params-resume_training/version_0

Path to Checkpointed files:
---------------------------
It includes the following directories: (i) path of TensorBoard logs files,
(ii) checkpoints. Following is an example:
tensorboard_logs/distilgpt2_params-resume_training/version_0/checkpoints

Name of Checkpointed files:
---------------------------
During training, the last epoch is always checkpointed in the file 'last.ckpt'. 
Additionally, epochs with the lowest validation loss are also checkpointed. The
names of these files includes the epoch number plus the value of the
validation loss. Following is an example:
last.ckpt, 'epoch=10-val_loss=0.23297.ckpt', 'epoch=22-val_loss=0.14760.ckpt'


parameters for file "ctbMain.py"
- 'save_top_k':    number of checkpointed files to save		Default: 1
- 'ckpt_filename': name of files that will be checkpointed
		      Default:'{epoch:02d}-{val_loss:.5f}'
- 'no_testing':    whether to not-test a trained model		Default: False
- 
{'save_top_k': 2, 'ckpt_filename': 'adam-e-02,ReduceLROnPlateau-pat=0,{epoch:02d}-{val_loss:.5f}', 'no_testing': True}


parameters for file "ctbModel.py"
- Parameters MUST be the same as those in the model from where training is resumed
- 'model_type':      'name of pretrained model plus name of training dataset'
- 'tokenizer_type':  'name of pretrained tokenizer plus name of training
                         dataset whose tokens are added'
- 'optz':            'name of optimizer'
- 'optz_params':     {parameters of optimizer EXCLUDING "params"}
- 'lr_sched':        'name of lr-scheduler'
- 'lr_sched_params': {parameters of scheduler EXCLUDING "optimizer"}
- 
{'model_type': 'distilgpt2-dstc2', 'tokenizer_type': 'gpt2-dstc2', 'optz': 'Adam', 'lr_sched': 'ReduceLROnPlateau', 'lr_sched_params': {'mode': 'min', 'patience': 5, 'verbose': True}} 


parameters for file "ctbData.py"
- 'default_format_path': 'path to training dataset'
- 
{'default_format_path': 'data/dialog-bAbI-tasks/dstc2/defaultFormat.train', 'batch_size': 2} 


parameters for Lightning Trainer
- For a list of parameters, see Trainer.__init__(...) in PyTorch Lightning documentation
- The Model and Training state is restored during "trainer.fit()", therefore
     "trainer.tune()" is useless. This means, do not use 'auto_lr_find' and 
     'auto_scale_batch_size'
- 
{'gpus': 1, 'resume_from_checkpoint': 'tensorboard_logs/distilgpt2_params/version_13/checkpoints/last.ckpt'}

