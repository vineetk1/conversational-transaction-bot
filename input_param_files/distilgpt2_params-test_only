'''
Vineet Kumar, sioom.ai

This input file has a unique set of user-settable hyper-parameters for
testing a checkpointed model. It is envisioned that there will be many input
files, in the "input_param_files" directory, each with their own unique set
of hyperparameters.

Note the following:
	(1) This file should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "conversational-transaction-bot"
 
Command-line:
-------------
python3 ctbMain.py input_param_files/distilgpt2_params-test_only 

Path to ctb logs files:
-----------------------
It is the default directory, and the name of the file is "ctb_logs".

Path to Lightning logs files:
-----------------------------
It includes the following directories: (i) ctb_lightning_logs, (ii) name
of this file, (iii) a unique version number that increases every time this
file is used for training. Following is an example:
ctb_lightning_logs/distilgpt2_params-test_only/version_0

Path to Checkpointed files:
---------------------------
It includes the following directories: (i) path of Lightning logs files,
(ii) checkpoints. Following is an example:
ctb_lightning_logs/distilgpt2_params-test_only/version_0/checkpoints

Name of Checkpointed files:
---------------------------
During training, the checkpointed files are those that have the lowest
validation loss. The name includes the epoch number plus the value of the
validation loss. Following is an example:
epoch=00-val_loss=0.2329.ckpt
'''


# parameters for file "ctbMain.py"
{'chkpt': 'ctb_lightning_logs/distilgpt2_params/version_0/checkpoints/epoch=02-val_loss=0.1540.ckpt', 'pass_fail_stat': False}

# parameters for file "ctbModel.py"
# Dictionary MUST be empty because these parameters are loaded from checkpoint file
{}

# parameters for file "ctbData.py"
# 'default_format_path': 'path to training dataset'
{'default_format_path': 'data/dialog-bAbI-tasks/dstc2/defaultFormat.train'} 

# parameters for Lightning Trainer
# For a list of parameters, see Trainer.__init__(...) in PyTorch Lightning documentation
{'gpus': 1}

