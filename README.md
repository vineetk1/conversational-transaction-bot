# Conversational transaction bot
The Conversational Transaction Bot executes transactions on user commands. A user initiates a dialogue with a text query to a bot. The bot understands the user text, initiates execution of operations at the backend, and responds to the user in text. The dialogue continues until, normally, the user terminates the dialogue when its requests have been serviced by the bot. The implementation is based on Deep Learning Transformers.
## Requirements
* PyTorch version >= 1.6.0
* Python version >= 3.8.5
* PyTorch-Lightning version used is 1.1.4
* Huggingface Transformers version used is 4.0.1
* Tensorboard version used is 2.4.1
## Installation
```
pip3 install transformers
pip3 install pytorch-lightning
pip3 install tensorboard
git clone https://github.com/vineetk1/conversational-transaction-bot.git
cd conversational-transaction-bot
```
Note that the default directory is *conversational-transaction-bot*. Unless otherwise stated, all commands from the command-line must be delivered from the default directory.
## Download DSTC2 dataset
1. Go to https://fb-public.app.box.com/s/chnq60iivzv5uckpvj2n2vijlyepze6w 
1. Download *dialog-bAbI-tasks_1_.tgz* in directory *data*  

Verify that the current working directory is *data*.    
```
tar zxvf dialog-bAbI-tasks_1_.tgz
rm dialog-bAbI-tasks_1_.tgz
```
Verify that the DSTC2 dataset is in the directory *data/dialog-bAbI-tasks*.   
## Convert DSTC2 dataset to the default format
Convert dataset into a default format. An example of the default format is shown in the file *convert_to_default_formats/default_format_example.md*.   

Verify that the current working directory is the default directory. Following command converts the downloaded dataset to the default format, and saves it in the files - *defaultFormat.train, defaultFormat.valid, defaultFormat.test* - of the directory *data/dialog-bAbI-tasks/dstc2*:
```
python3 convert_to_default_formats/dstc2_to_defaultFormat.py
```
Note that the above program converts the DSTC2 dataset to the default format. A new conversion program will have to be implemented for a dataset that has a different format from that of the DSTC2 dataset. 
## Train, validate, and test a model
Following command trains a model, saves checkpoints that have the lowest validation loss, runs the test dataset on the checkpointed model with the lowest validation loss, and outputs a Perplexity value of the model:
```
python3 ctbMain.py input_param_files/distilgpt2_params
```
The user-settable hyper-parameters are in the file *input_param_files/distilgpt2_params*. A list of all the hyper-parameters is in the <a href="https://www.pytorchlightning.ai" target="_blank">PyTorch-Lightning documentation</a>, and any hyper-parameter can be used.    
To assist in Training, the two parameters *auto_lr_find* and *auto_scale_batch_size* in the file *input_param_files/distilgpt2_params* enable the software to automatically find an initial Learning-Rate and a Batch-Size respectively.    
As training progresses, graphs of *"training-loss vs. epoch #"*, and *"validation-loss vs. epoch #"* are plotted in real-time using TensorBoard as follows:
     
<img src=images/train_loss_epoch.png width=400 height=250> <img src=images/val_loss_epoch.png width=400 height=250>    
Training is stopped by typing, at the commandline, the keystroke ctrl-c. The current training information is checkpointed, and training stops. Training can be resumed, at some future time, from the checkpointed file.
## Resume training, validation, and testing a model
Training is resumed with the following command:
```
python3 ctbMain.py input_param_files/distilgpt2_params-resume_training
```
The user-settable hyper-parameters are in the file *input_param_files/distilgpt2_params-resume_training*.  
## Load and test a model
A checkpointed model is loaded and tested with the following command:
```
python3 ctbLoadTest.py input_param_files/distilgpt2_params-test_only 
```
The Perplexity of the model is calculated from the test dataset. The user-settable hyper-parameters are in the file *input_param_files/distilgpt2_params-test_only*. A detailed statistics of the model is generated by setting the parameter *pass_fail_stat*. Two files, namely, *failed_dialogs_stat.txt* and *passed_dialogs_stat.txt* are created. The *failed_dialogs_stat.txt* file has information about all the dialogs that failed, and *passed_dialogs_stat.txt* has information about all the dialogs that passed.
## Interact with the deployed model
Work In Progress.
## Fine-tuning Distilgpt2 with DSTC2 dataset
#### &emsp; &emsp; Graph: Validation-loss vs. Epoch for varying Learning-Rates
<img src=images/tensorboard,val_loss-5_epochs,nag.png width=800 height=500>     
**Hyperparameters:**    
&emsp; &emsp; Optimizer Parameters -- SGD, lr: variable, momentum:0.9, weight_decay: 0, dampening: 0 nesterov: True    
&emsp; &emsp; LR-Scheduler Parameters -- None    
&emsp; &emsp; "Color of curve, Learning-Rate, Val-loss at epoch 0" (en =euler number = 2.71828) -- Aqua, en<sup>-12</sup>, 1.69; &emsp; Dark Red, en<sup>-11</sup>, 1.15    
&emsp; &emsp; &emsp; &emsp; Blue, en<sup>-10</sup>, 0.814; &emsp; Pink, en<sup>-6</sup>, 0.6802; &emsp; Orange, en<sup>-9</sup>, 0.6443; &emsp; Green, en<sup>-5</sup>, 0.6143; &emsp; Grey, en<sup>-8</sup>, 0.5248; &emsp; Green, en<sup>-7</sup>, 0.4954    
